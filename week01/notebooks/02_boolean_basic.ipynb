{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Basic Boolean Search in Documents\n",
    "\n",
    "## Objective\n",
    "Expand the simple term search functionality to include Boolean search capabilities. This will allow users to perform more complex queries by combining multiple search terms using Boolean operators.\n",
    "\n",
    "## Problem Description\n",
    "You must enhance the existing search engine from the previous exercise to support Boolean operators: AND, OR, and NOT. This will enable the retrieval of documents based on the logical relationships between multiple terms.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "### Step 1: Update Data Preparation\n",
    "Ensure that the documents are still loaded and preprocessed from the previous task. The data should be clean and ready for advanced querying.\n",
    "\n",
    "### Step 2: Create an Inverted Index\n",
    "\n",
    "Create an inverted index from the documents. This index maps each word to the set of document IDs in which that word appears. This facilitates word lookup in the search process.\n",
    "\n",
    "### Step 3: Query Processing\n",
    "- **Parse the Query**: Implement a function to parse the input query to identify the terms and operators.\n",
    "- **Search Documents**: Based on the parsed query, implement the logic to retrieve and rank the documents according to the Boolean expressions.\n",
    "\n",
    "### Step 4: Displaying Results\n",
    "- **Output the Results**: Display the documents that match the query criteria. Include functionalities to handle queries that result in no matching documents.\n",
    "\n",
    "## Evaluation Criteria\n",
    "- **Correctness**: The Boolean search implementation should correctly interpret and process the queries according to the Boolean logic.\n",
    "- **Efficiency**: Consider the efficiency of your search process, especially as the complexity of queries increases.\n",
    "- **User Experience**: Ensure that the interface for inputting queries and viewing results is user-friendly.\n",
    "\n",
    "This exercise will deepen your understanding of how search engines process and respond to user queries."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3678b6ceab34c216"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Excercise",
   "id": "bf6137fe05e9ddd2"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re"
   ],
   "id": "a639272ac64a8ce2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get the total of books",
   "id": "5eaf1d8116e9975a"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "DIR = \"../data\"\n",
    "total_books = len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))])\n",
    "print(total_books)"
   ],
   "id": "351c3865ae5d5a00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Helper function to flatten arrays",
   "id": "4516ff570aa3f79b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]"
   ],
   "id": "db0748521da82bf6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Helper function to read the book content",
   "id": "c576b89e3146d7e0"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def get_book_content(book_path: str) -> str:\n",
    "    with open(f\"{DIR}/{book_path}\") as file:\n",
    "        book_content = file.read()\n",
    "        return book_content"
   ],
   "id": "fcade3ae095b338",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Regex to filter out punctuation and special characters\n",
    "\n",
    "Empty list to store the content of all the books"
   ],
   "id": "956128a86e3d9776"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "punctuation_regex = r\"[^\\w\\s]\"\n",
    "books = []"
   ],
   "id": "f0986dc609e5a812",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Function to remove all non-alphanumeric characters and convert to lower case",
   "id": "7576cdfa06bc97f7"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_tokens(tokens: list[str]) -> list[str]:\n",
    "    normalized_tokens = [re.sub(punctuation_regex, \"\", str.lower()) for str in tokens]\n",
    "    return normalized_tokens"
   ],
   "id": "7963928b994ea30d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Function to tokenize the book's content",
   "id": "84ff237570984a41"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_input(input: str) -> list[str]:\n",
    "    tokens = normalize_tokens(input.split())\n",
    "    return tokens"
   ],
   "id": "180774738f2b535b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Function to tokenize all books"
   ],
   "id": "6d7cae58ba7c8856"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_books() -> list[str]:\n",
    "    book_tokens = []\n",
    "    for i in range(1, total_books + 1):\n",
    "        book_path = f\"{DIR}/book{i}.txt\"\n",
    "        book_content = get_book_content(book_path).lower()\n",
    "        books.append(book_content)\n",
    "        book_tokens.append(tokenize_input(book_content))\n",
    "\n",
    "    return book_tokens"
   ],
   "id": "d6e6164fdd164d13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We fill a rows array with the filename of our books",
   "id": "9863870d47c25864"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "rows = [\"\"]\n",
    "for i in range(1, total_books + 1):\n",
    "    rows.append(f\"book{i}\")"
   ],
   "id": "65fb188e62c92d89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Then we call the tokenize_books function to store the tokens of every book in a \"tokens\" list",
   "id": "90e69cd3689fb4bc"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "tokens = tokenize_books()",
   "id": "10ca4a3d9b24829",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We filter the tokens and store only the unique tokens. We use the flatten function to accomplish this",
   "id": "b46863bbec7cf860"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "unique_tokens = set(flatten(tokens))",
   "id": "d182e9f67aa1d811",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We initialize the columns array to store a dictionary with the token and a list of 0 and 1's indicating if the word was found in the book content",
   "id": "991cc83e935ae83d"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "columns = [{\"token\": str, \"appereances\": list[int]}]",
   "id": "a009eda035778aad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "With this code block we construct the index.\n",
    "- First we loop through the unique tokens array to populate the columns array\n",
    "- Then we loop through the books to check if the token is contained in the book content\n",
    "- If the token is found, we append a 1 to the columns \"appereances\" value \n",
    "- If is not found we append a 0\n"
   ],
   "id": "35c474b047ae3570"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "for index, token in enumerate(unique_tokens):\n",
    "    appereances = []\n",
    "    columns.append({\"token\": token, \"appereances\": appereances})\n",
    "    for i, book in enumerate(books):\n",
    "        if token in book:\n",
    "            appereances.append(1)\n",
    "            columns[index][\"appereances\"] = appereances\n",
    "        else:\n",
    "            appereances.append(0)\n",
    "            columns[index][\"appereances\"] = appereances\n",
    "\n",
    "regular_index = {\"books\": rows, \"tokens\": columns}"
   ],
   "id": "47042d4a48b31a28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We print the results",
   "id": "92b0c82fe0155880"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(regular_index[\"books\"]))\n",
    "print(regular_index[\"books\"])"
   ],
   "id": "a7094ac1d7dabfab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "print(len(regular_index[\"tokens\"]))",
   "id": "bf120ba7f7f6bd88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "print(regular_index[\"tokens\"])",
   "id": "75df43fba6ff1e4d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.636763Z",
     "start_time": "2024-05-15T06:24:02.632865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import math"
   ],
   "id": "79f704d152316e88",
   "outputs": [],
   "execution_count": 2014
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We get the total files of the corpus",
   "id": "41b3aab42e1e3d47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.690962Z",
     "start_time": "2024-05-15T06:24:02.687909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DIR = \"./data/textos\"\n",
    "total_files = len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))])\n",
    "print(total_files)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "execution_count": 2015
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Helper function to flaten lists",
   "id": "e9c89c0bdfab0ba7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.701750Z",
     "start_time": "2024-05-15T06:24:02.699864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]"
   ],
   "id": "c461917ea6ebbf73",
   "outputs": [],
   "execution_count": 2016
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.705069Z",
     "start_time": "2024-05-15T06:24:02.703368Z"
    }
   },
   "cell_type": "code",
   "source": "punctuation_regex = r\"[^\\w\\s]\"",
   "id": "d2adeb7ab27b3aae",
   "outputs": [],
   "execution_count": 2017
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Helper functions to normalize and tokenize strings",
   "id": "4bded0268ff21d55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.712005Z",
     "start_time": "2024-05-15T06:24:02.710052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_tokens(tokens: list[str]) -> list[str]:\n",
    "    normalized_tokens = [re.sub(punctuation_regex, \"\", str.lower()) for str in tokens]\n",
    "    return normalized_tokens"
   ],
   "id": "2282a5c27bb39ac6",
   "outputs": [],
   "execution_count": 2018
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.723447Z",
     "start_time": "2024-05-15T06:24:02.721705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_input(input: str) -> list[str]:\n",
    "    tokens = normalize_tokens(input.split())\n",
    "    return tokens"
   ],
   "id": "25d3987ab6eb27a3",
   "outputs": [],
   "execution_count": 2019
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.727313Z",
     "start_time": "2024-05-15T06:24:02.725001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_content() -> list[str]:\n",
    "    file_tokens = []\n",
    "    for i in range(1, total_files + 1):\n",
    "        with open(f\"{DIR}/texto-{i}.txt\") as file:\n",
    "            file_content = file.read()\n",
    "        file_tokens.append(tokenize_input(file_content))\n",
    "\n",
    "    return file_tokens"
   ],
   "id": "4ad6a97c97db5e38",
   "outputs": [],
   "execution_count": 2020
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.742065Z",
     "start_time": "2024-05-15T06:24:02.733586Z"
    }
   },
   "cell_type": "code",
   "source": "tokens = tokenize_content()",
   "id": "58f4b5fb7fd82e89",
   "outputs": [],
   "execution_count": 2021
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We get the all the unique tokens of the corpus",
   "id": "abf1d5acb854c332"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.748587Z",
     "start_time": "2024-05-15T06:24:02.746495Z"
    }
   },
   "cell_type": "code",
   "source": "unique_tokens = set(flatten(tokens))",
   "id": "71630099d88c11d2",
   "outputs": [],
   "execution_count": 2022
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.763127Z",
     "start_time": "2024-05-15T06:24:02.761453Z"
    }
   },
   "cell_type": "code",
   "source": "#print(unique_tokens)",
   "id": "9dc3c6808de03909",
   "outputs": [],
   "execution_count": 2023
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.766234Z",
     "start_time": "2024-05-15T06:24:02.764638Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(unique_tokens))",
   "id": "13114595eb8a90c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n"
     ]
    }
   ],
   "execution_count": 2024
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here we build the first token matrix\n",
    "\n",
    "Using a dictionary we store the text name as the key and a counter of how many times a token is found on the text as the value"
   ],
   "id": "a11e4fba9a7b5cfa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.787509Z",
     "start_time": "2024-05-15T06:24:02.770575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens_matrix = {}\n",
    "\n",
    "for i in range(1, total_files + 1):\n",
    "    with open(f\"{DIR}/texto-{i}.txt\") as file:\n",
    "        file_name = f\"texto-{i}.txt\"\n",
    "        tokens_matrix[file_name] = []\n",
    "        file_content = file.read()\n",
    "        file_words = file_content.split()\n",
    "        normalized_words = normalize_tokens(file_words)\n",
    "        # print(normalized_words)\n",
    "\n",
    "        for index, token in enumerate(unique_tokens):\n",
    "            tokens_matrix[file_name].append(0)\n",
    "            count = 0\n",
    "            for word in normalized_words:\n",
    "                if word == token:\n",
    "                    count += 1\n",
    "                    tokens_matrix[file_name][index] = count\n",
    "                    #print(word, count)"
   ],
   "id": "58f644e18b4d96be",
   "outputs": [],
   "execution_count": 2025
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.790169Z",
     "start_time": "2024-05-15T06:24:02.788481Z"
    }
   },
   "cell_type": "code",
   "source": "#print(tokens_matrix)",
   "id": "90baf767f604c1b1",
   "outputs": [],
   "execution_count": 2026
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We export the resulting matrix as tokens_matrix.csv",
   "id": "ee100c0495a7c3a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.800106Z",
     "start_time": "2024-05-15T06:24:02.790857Z"
    }
   },
   "cell_type": "code",
   "source": "pd.DataFrame.from_dict(data=tokens_matrix, orient='index').to_csv('tokens_matrix.csv', header=list(unique_tokens))",
   "id": "62e391ef2ef2a33",
   "outputs": [],
   "execution_count": 2027
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We get the number of querys in total",
   "id": "85a984a77ca89b1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.803817Z",
     "start_time": "2024-05-15T06:24:02.801537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "QUERY_DIR = \"./data/querys\"\n",
    "total_querys = len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))])\n",
    "print(total_querys)"
   ],
   "id": "6d6e1bf69ba7dde8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "execution_count": 2028
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.806261Z",
     "start_time": "2024-05-15T06:24:02.804453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_names = []\n",
    "for i in range(1, total_querys + 1):\n",
    "    query_names.append(f\"query-{i}\")\n",
    "\n",
    "print(query_names)"
   ],
   "id": "ab5c41bf2fe98d76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['query-1', 'query-2', 'query-3', 'query-4', 'query-5', 'query-6', 'query-7', 'query-8', 'query-9', 'query-10', 'query-11', 'query-12', 'query-13', 'query-14', 'query-15', 'query-16', 'query-17', 'query-18', 'query-19', 'query-20']\n"
     ]
    }
   ],
   "execution_count": 2029
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we build our query matrix\n",
    "\n",
    "We check if the content of the query is in our unique_tokens list\n"
   ],
   "id": "5811d31ca34b7f4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.817509Z",
     "start_time": "2024-05-15T06:24:02.806812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_matrix = {}\n",
    "\n",
    "for i in range(1, total_querys + 1):\n",
    "    with open(f\"{QUERY_DIR}/query-{i}.txt\") as file:\n",
    "        query_name = f\"query-{i}\"\n",
    "        query_matrix[query_name] = []\n",
    "        file_content = file.read()\n",
    "        file_words = file_content.split()\n",
    "        normalized_words = normalize_tokens(file_words)\n",
    "        # print(normalized_words)\n",
    "\n",
    "        for index, token in enumerate(unique_tokens):\n",
    "            query_matrix[query_name].append(0)\n",
    "            for word in normalized_words:\n",
    "                if word == token:\n",
    "                    query_matrix[query_name][index] = 1\n"
   ],
   "id": "74e64085a39b0952",
   "outputs": [],
   "execution_count": 2030
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.820410Z",
     "start_time": "2024-05-15T06:24:02.818264Z"
    }
   },
   "cell_type": "code",
   "source": "#print(query_matrix)",
   "id": "ffca3550445ffdf2",
   "outputs": [],
   "execution_count": 2031
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Export the query matrix as query_matrix.csv",
   "id": "eb8e964dfe4aad95"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.830752Z",
     "start_time": "2024-05-15T06:24:02.822898Z"
    }
   },
   "cell_type": "code",
   "source": "pd.DataFrame.from_dict(data=query_matrix, orient='index').to_csv('query_matrix.csv', header=list(unique_tokens))",
   "id": "32887f6273350313",
   "outputs": [],
   "execution_count": 2032
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Function to calculate the distance between two vectors of same dimension",
   "id": "97ef232f82357ecb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.834784Z",
     "start_time": "2024-05-15T06:24:02.832379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_distance(token_vector, query_vector):\n",
    "    numerator = 0\n",
    "    token_module = 0\n",
    "    query_module = 0\n",
    "    for i in range(len(token_vector)):\n",
    "        numerator += token_vector[i] * query_vector[i]\n",
    "        token_module += token_vector[i] ** 2\n",
    "        query_module += query_vector[i] ** 2\n",
    "\n",
    "    denominator = math.sqrt(token_module) * math.sqrt(query_module)\n",
    "    distance = numerator / denominator\n",
    "\n",
    "    return distance"
   ],
   "id": "aec5f6f6370bceb5",
   "outputs": [],
   "execution_count": 2033
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We build the result matrix with the corresponding distances between every query and every text",
   "id": "226ab88d9fab7959"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.897615Z",
     "start_time": "2024-05-15T06:24:02.835740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_matrix = {}\n",
    "\n",
    "for i in range(1, total_files + 1):\n",
    "    text_name = f\"texto-{i}.txt\"\n",
    "    results_matrix[text_name] = []\n",
    "    for j in range(1, total_querys + 1):\n",
    "        query_name = f\"query-{j}\"\n",
    "        results_matrix[text_name].append(\n",
    "            get_distance(token_vector=tokens_matrix[text_name], query_vector=query_matrix[query_name]))"
   ],
   "id": "c69bbed8047b6c58",
   "outputs": [],
   "execution_count": 2034
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.900503Z",
     "start_time": "2024-05-15T06:24:02.898708Z"
    }
   },
   "cell_type": "code",
   "source": "#print(results_matrix)",
   "id": "d582ed9541a4368a",
   "outputs": [],
   "execution_count": 2035
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Export the results matrix as result_matrix.csv",
   "id": "caa2f02dd0075f12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.905029Z",
     "start_time": "2024-05-15T06:24:02.901487Z"
    }
   },
   "cell_type": "code",
   "source": "pd.DataFrame.from_dict(data=results_matrix, orient='index').to_csv('results_matrix.csv', header=query_names)",
   "id": "7729be449d098089",
   "outputs": [],
   "execution_count": 2036
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we construct the ranking dictionary to rank the 3 more relevant texts to every query",
   "id": "e595b2b0f7feff9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.908644Z",
     "start_time": "2024-05-15T06:24:02.906212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ranking_dict = {}\n",
    "\n",
    "for i in range(1, total_files + 1):\n",
    "    current_query = f\"query-{i}\"\n",
    "    current_text = f\"texto-{i}.txt\"\n",
    "    ranking_dict[current_query] = []\n",
    "    for j in range(3):\n",
    "        best_result_index = results_matrix[current_text].index(max(results_matrix[current_text]))\n",
    "\n",
    "        ranking_dict[current_query].append(\n",
    "            f\"texto-{best_result_index + 1}\" + \" d=\" + str(results_matrix[current_text][best_result_index]))\n",
    "\n",
    "        results_matrix[current_text].pop(best_result_index)\n"
   ],
   "id": "77cacc0300b64c68",
   "outputs": [],
   "execution_count": 2037
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.910782Z",
     "start_time": "2024-05-15T06:24:02.909374Z"
    }
   },
   "cell_type": "code",
   "source": "#print(ranking_dict)",
   "id": "616e3d0ce967054",
   "outputs": [],
   "execution_count": 2038
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Export the ranking dict as ranking.csv",
   "id": "1441b8f9dfe04adc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T06:24:02.913828Z",
     "start_time": "2024-05-15T06:24:02.911431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pd.DataFrame.from_dict(data=ranking_dict, orient='index').to_csv('ranking.csv',\n",
    "                                                                 header=[\"Primero\", \"Segundo\", \"Tercero\"])"
   ],
   "id": "a8fc0ef8eaaf3994",
   "outputs": [],
   "execution_count": 2039
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
